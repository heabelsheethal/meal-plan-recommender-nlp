{"cells":[{"cell_type":"markdown","metadata":{"id":"gjp3xSmzpg4-"},"source":["Using Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":920,"status":"ok","timestamp":1753815062183,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"},"user_tz":240},"id":"Ag-eWmq_pcR6","outputId":"c83b0e68-bd6c-444d-ce0b-ba39166140c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive/Users/celinewidjaja/Documents/recipe-reccomender/full_code.ipynb\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aP7p_P2zpsio","outputId":"e8254de2-109d-4a0e-9102-c2973f8b6983","executionInfo":{"status":"ok","timestamp":1753807927755,"user_tz":240,"elapsed":13960,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"]}],"source":["# Import Libraries\n","!pip install nltk spacy unidecode gensim\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","from unidecode import unidecode\n","from gensim.models import Word2Vec\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EO9Z7F6GpyUF","executionInfo":{"status":"ok","timestamp":1753815091835,"user_tz":240,"elapsed":7044,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"}},"outputId":"f03793ae-f415-4cbd-81bd-d089e893ef88","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["# Download NLTK and spaCy models\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","import spacy\n","spacy.cli.download(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65070,"status":"ok","timestamp":1753815158592,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"},"user_tz":240},"id":"j8oS-ZnZq0r7","outputId":"c7e79fcb-1167-4bad-e873-e4ae0cef8a18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: (996675, 11)\n"]}],"source":["# Load data from Google Drive\n","file_path = \"/content/drive/MyDrive/NLP - Group Project/preprocessed_recipes.csv\"\n","\n","# Load the dataset\n","df_preprocessed = pd.read_csv(file_path)\n","print(\"Original shape:\", df_preprocessed.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6HtItHXxYV2"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","class RecipeCorpus:\n","    def __init__(self, df, batch_size=100000):\n","        self.df = df\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","        for start in range(0, len(self.df), self.batch_size):\n","            end = start + self.batch_size\n","            batch = self.df.iloc[start:end]\n","            for _, row in batch.iterrows():\n","                ingredients = str(row['clean_ingredients'])\n","                directions = str(row['clean_directions'])\n","                yield (ingredients + ' ' + directions).split()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypp4kMV9q7bK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753817653699,"user_tz":240,"elapsed":2434289,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"}},"outputId":"e7f6a34d-6b04-4a90-e198-66ed2f8ea84e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(864154130, 1137149380)"]},"metadata":{},"execution_count":17}],"source":["corpus = RecipeCorpus(df_preprocessed)\n","\n","w2v_model = Word2Vec(vector_size=100, window=5, min_count=2, workers=4, seed=42)\n","w2v_model.build_vocab(corpus)\n","\n","# Train using corpus\n","w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJYnGUGkx7tk"},"outputs":[],"source":["# Cooking Time Extraction\n","def extract_cook_time(text):\n","    matches = re.findall(r'(\\d+)\\s*(minutes|min|hours|hrs)', text.lower())\n","    total_minutes = 0\n","    for num, unit in matches:\n","        num = int(num)\n","        if 'hour' in unit or 'hr' in unit:\n","            total_minutes += num * 60\n","        else:\n","            total_minutes += num\n","\n","    # Set to max 6 hours (360 min)\n","    total_minutes = min(total_minutes, 360)\n","\n","    # Use a default if no time was found\n","    if total_minutes == 0:\n","        total_minutes = 30\n","    hours = total_minutes // 60\n","    minutes = total_minutes % 60\n","    return f\"{hours} hr {minutes} min\" if hours > 0 else f\"{minutes} min\"\n","\n","df_preprocessed['cooking_time'] = df_preprocessed['directions'].apply(extract_cook_time)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ducDYWCerc9D"},"outputs":[],"source":["# Cuisine Tagging\n","cuisine_keywords = {\n","    \"Italian\": {\"parmesan\", \"mozzarella\", \"basil\", \"oregano\", \"pasta\", \"risotto\", \"lasagna\"},\n","    \"Mexican\": {\"tortilla\", \"salsa\", \"jalapeno\", \"chipotle\", \"queso\", \"avocado\", \"enchilada\"},\n","    \"Indian\": {\"turmeric\", \"curry\", \"cumin\", \"masala\", \"paneer\", \"garam\", \"dal\", \"coriander\"},\n","    \"Chinese\": {\"soy\", \"ginger\", \"scallion\", \"noodle\", \"tofu\", \"hoisin\", \"bok choy\"},\n","    \"Japanese\": {\"miso\", \"sushi\", \"nori\", \"sake\", \"wasabi\", \"teriyaki\", \"mirin\"},\n","    \"Middle Eastern\": {\"couscous\", \"hummus\", \"tahini\", \"zaatar\", \"sumac\", \"falafel\"},\n","    \"Thai\": {\"lemongrass\", \"curry\", \"coconut\", \"fish sauce\", \"galangal\", \"kaffir\"},\n","    \"American\": {\n","            \"steak\", \"grill\", \"bbq\", \"barbecue\", \"cheddar\", \"ranch\", \"macaroni\",\n","            \"sloppy joe\", \"brisket\", \"meatloaf\", \"cornbread\", \"bacon\", \"buttermilk\",\n","            \"burger\", \"fried chicken\", \"coleslaw\", \"hotdog\", \"fries\", \"ketchup\"\n","    },\n","    \"French\": {\"thyme\", \"brie\", \"wine\", \"butter\", \"shallot\", \"tarragon\", \"ratatouille\"},\n","    \"Indonesian\": {\"tempeh\", \"sambal\", \"gula\", \"rendang\", \"ketumbar\", \"kecap\",\"Medan\"},\n","    \"German\": {\"sauerkraut\", \"strudel\", \"pumpernickel\", \"bratwurst\", \"spätzle\"},\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTXlbCdarlme"},"outputs":[],"source":["# Ensure all text fields are string and fill NaNs\n","df_preprocessed['clean_ingredients'] = df_preprocessed['clean_ingredients'].fillna(\"\").astype(str)\n","\n","# Function to calculate average word vector\n","def get_avg_vector(words):\n","    vectors = [w2v_model.wv[w] for w in words if w in w2v_model.wv]\n","    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n","\n","# Calculate cuisine keyword vectors\n","cuisine_vectors = {\n","    cuisine: get_avg_vector(list(keywords))\n","    for cuisine, keywords in cuisine_keywords.items()\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44EgmnvBro-E"},"outputs":[],"source":["# Tagging cuisine based on cosine similarity\n","def tag_cuisine(text):\n","    text = str(text)  # Convert to string if not already\n","    tokens = text.split()\n","    vec = get_avg_vector(tokens)\n","\n","    similarities = {\n","        cuisine: np.dot(vec, cuisine_vectors[cuisine]) /\n","                 (np.linalg.norm(vec) * np.linalg.norm(cuisine_vectors[cuisine]) + 1e-10)\n","        for cuisine in cuisine_vectors\n","    }\n","\n","    return max(similarities, key=similarities.get)\n","\n","# Apply cuisine tagging\n","df_preprocessed['cuisine_tag'] = df_preprocessed['clean_ingredients'].apply(tag_cuisine)"]},{"cell_type":"code","source":["# Cuisine Overrides\n","cuisine_overrides = {\n","    # American\n","    \"steak\": \"American\",\n","    \"brisket\": \"American\",\n","    \"sloppy joe\": \"American\",\n","    \"bbq\": \"American\",\n","    \"barbecue\": \"American\",\n","    \"pie crust\": \"American\",\n","    \"ranch\": \"American\",\n","    \"bacon\": \"American\",\n","    \"meatloaf\": \"American\",\n","    \"macaroni\": \"American\",\n","    \"cheddar\": \"American\",\n","    \"hotdog\": \"American\",\n","    \"cornbread\": \"American\",\n","    \"buttermilk\": \"American\",\n","    \"fried chicken\": \"American\",\n","    \"chili\": \"American\",\n","    \"ketchup\": \"American\",\n","    \"coleslaw\": \"American\",\n","    \"graham cracker\": \"American\",\n","    \"cranberry sauce\": \"American\",\n","    \"peanut butter\": \"American\",\n","    \"crisco\": \"American\",\n","\n","    # Italian\n","    \"pasta\": \"Italian\",\n","    \"lasagna\": \"Italian\",\n","    \"risotto\": \"Italian\",\n","    \"bolognese\": \"Italian\",\n","    \"mozzarella\": \"Italian\",\n","    \"parmesan\": \"Italian\",\n","    \"oregano\": \"Italian\",\n","    \"basil\": \"Italian\",\n","    \"anchovy\": \"Italian\",\n","    \"fettuccine\": \"Italian\",\n","    \"gnocchi\": \"Italian\",\n","    \"prosciutto\": \"Italian\",\n","    \"carbonara\": \"Italian\",\n","    \"caprese\": \"Italian\",\n","    \"marinara\": \"Italian\",\n","    \"penne\": \"Italian\",\n","    \"ziti\": \"Italian\",\n","\n","    # French\n","    \"tart\": \"French\",\n","    \"quiche\": \"French\",\n","    \"ratatouille\": \"French\",\n","    \"vinaigrette\": \"French\",\n","    \"brie\": \"French\",\n","    \"croissant\": \"French\",\n","    \"thyme\": \"French\",\n","    \"shallot\": \"French\",\n","    \"crepe\": \"French\",\n","    \"bouillabaisse\": \"French\",\n","    \"bechamel\": \"French\",\n","    \"duxelles\": \"French\",\n","    \"souffle\": \"French\",\n","\n","    # German\n","    \"pretzel\": \"German\",\n","    \"portzelky\": \"German\",\n","    \"strudel\": \"German\",\n","    \"sauerkraut\": \"German\",\n","    \"bratwurst\": \"German\",\n","    \"spätzle\": \"German\",\n","    \"wiener\": \"German\",\n","\n","    # Indonesian\n","    \"rendang\": \"Indonesian\",\n","    \"sambal\": \"Indonesian\",\n","    \"medan\": \"Indonesian\",\n","    \"tempeh\": \"Indonesian\",\n","    \"gula jawa\": \"Indonesian\",\n","    \"kecap\": \"Indonesian\",\n","    \"nasi goreng\": \"Indonesian\",\n","    \"bumbu\": \"Indonesian\",\n","    \"ikan bakar\": \"Indonesian\",\n","    \"balado\": \"Indonesian\",\n","\n","    # Mexican\n","    \"tortilla\": \"Mexican\",\n","    \"quesadilla\": \"Mexican\",\n","    \"ranchero\": \"Mexican\",\n","    \"enchilada\": \"Mexican\",\n","    \"salsa\": \"Mexican\",\n","    \"jalapeno\": \"Mexican\",\n","    \"chipotle\": \"Mexican\",\n","    \"avocado\": \"Mexican\",\n","    \"taco\": \"Mexican\",\n","    \"guacamole\": \"Mexican\",\n","    \"refried beans\": \"Mexican\",\n","    \"queso\": \"Mexican\",\n","\n","    # Japanese\n","    \"nori\": \"Japanese\",\n","    \"miso\": \"Japanese\",\n","    \"sushi\": \"Japanese\",\n","    \"teriyaki\": \"Japanese\",\n","    \"sashimi\": \"Japanese\",\n","    \"mirin\": \"Japanese\",\n","    \"wasabi\": \"Japanese\",\n","    \"dashi\": \"Japanese\",\n","    \"udon\": \"Japanese\",\n","    \"tempura\": \"Japanese\",\n","\n","    # Middle Eastern\n","    \"falafel\": \"Middle Eastern\",\n","    \"tahini\": \"Middle Eastern\",\n","    \"zaatar\": \"Middle Eastern\",\n","    \"sumac\": \"Middle Eastern\",\n","    \"hummus\": \"Middle Eastern\",\n","    \"couscous\": \"Middle Eastern\",\n","    \"shawarma\": \"Middle Eastern\",\n","    \"pita\": \"Middle Eastern\",\n","    \"lamb kebab\": \"Middle Eastern\",\n","\n","    # Chinese\n","    \"soy sauce\": \"Chinese\",\n","    \"hoisin\": \"Chinese\",\n","    \"bok choy\": \"Chinese\",\n","    \"scallion\": \"Chinese\",\n","    \"dumpling\": \"Chinese\",\n","    \"wonton\": \"Chinese\",\n","    \"chow mein\": \"Chinese\",\n","    \"szechuan\": \"Chinese\",\n","    \"five-spice\": \"Chinese\",\n","    \"ginger\": \"Chinese\",\n","\n","    # Thai\n","    \"lemongrass\": \"Thai\",\n","    \"galangal\": \"Thai\",\n","    \"kaffir\": \"Thai\",\n","    \"coconut milk\": \"Thai\",\n","    \"fish sauce\": \"Thai\",\n","    \"green curry\": \"Thai\",\n","    \"red curry\": \"Thai\",\n","    \"thai basil\": \"Thai\",\n","\n","    # Indian\n","    \"masala\": \"Indian\",\n","    \"garam\": \"Indian\",\n","    \"paneer\": \"Indian\",\n","    \"dal\": \"Indian\",\n","    \"turmeric\": \"Indian\",\n","    \"curry\": \"Indian\",\n","    \"tikka\": \"Indian\",\n","    \"ghee\": \"Indian\",\n","    \"biryani\": \"Indian\"\n","}\n","\n","\n","\n","def tag_cuisine_with_overrides(text):\n","    text = str(text).lower()\n","    for keyword, cuisine in cuisine_overrides.items():\n","        if keyword in text:\n","            return cuisine\n","    tokens = text.split()\n","    vec = get_avg_vector(tokens)\n","    similarities = {\n","        cuisine: np.dot(vec, cuisine_vectors[cuisine]) /\n","                 (np.linalg.norm(vec) * np.linalg.norm(cuisine_vectors[cuisine]) + 1e-10)\n","        for cuisine in cuisine_vectors\n","    }\n","    return max(similarities, key=similarities.get)\n","\n","df_preprocessed['cuisine_tag'] = df_preprocessed['clean_ingredients'].apply(tag_cuisine_with_overrides)\n"],"metadata":{"id":"yjsSGMi9zPzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qgFSDUjsQoI"},"outputs":[],"source":["# Diet Tagging\n","diet_keywords = {\n","    \"Meat\": [\"beef\", \"chicken\", \"pork\", \"bacon\", \"sausage\", \"lamb\", \"turkey\", \"ham\", \"steak\"],\n","    \"Dairy\": [\"milk\", \"cheese\", \"butter\", \"cream\", \"yogurt\", \"egg\", \"ghee\"],\n","    \"Gluten\": [\"wheat\", \"barley\", \"rye\", \"flour\", \"bread\", \"pasta\", \"cracker\", \"noodle\"]\n","}\n","\n","def tag_diet(text):\n","    text = str(text).lower()\n","    has_meat = any(x in text for x in diet_keywords[\"Meat\"])\n","    has_dairy = any(x in text for x in diet_keywords[\"Dairy\"])\n","    has_gluten = any(x in text for x in diet_keywords[\"Gluten\"])\n","\n","    if has_meat:\n","        diet = \"Non-Vegetarian\"\n","    elif has_dairy:\n","        diet = \"Vegetarian\"\n","    else:\n","        diet = \"Vegan\"\n","\n","    gluten = \"Contains Gluten\" if has_gluten else \"Gluten-Free\"\n","    return f\"{diet}, {gluten}\"\n","\n","# Apply in batch (vectorized)\n","df_preprocessed['diet_tag'] = df_preprocessed['clean_ingredients'].apply(tag_diet)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1753822090202,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"},"user_tz":240},"id":"iKNtaA4dskOp","outputId":"8e667b6a-d7ac-46af-8474-983a716783b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source', 'NER', 'clean_ingredients', 'clean_directions', 'clean_text', 'simhash', 'cooking_time', 'cuisine_tag', 'diet_tag']\n"]}],"source":["print(df_preprocessed.columns.tolist())"]},{"cell_type":"code","source":["# Preview\n","df_preprocessed[['title', 'clean_ingredients', 'cuisine_tag', 'diet_tag', 'cooking_time']].head().style.set_properties(**{'white-space': 'pre-wrap'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"HfIHLW122Ozr","executionInfo":{"status":"ok","timestamp":1753822156542,"user_tz":240,"elapsed":253,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"}},"outputId":"75481218-de9c-4bae-cdff-2a3aecf659da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7ca8fe5ecf10>"],"text/html":["<style type=\"text/css\">\n","#T_e12c0_row0_col0, #T_e12c0_row0_col1, #T_e12c0_row0_col2, #T_e12c0_row0_col3, #T_e12c0_row0_col4, #T_e12c0_row1_col0, #T_e12c0_row1_col1, #T_e12c0_row1_col2, #T_e12c0_row1_col3, #T_e12c0_row1_col4, #T_e12c0_row2_col0, #T_e12c0_row2_col1, #T_e12c0_row2_col2, #T_e12c0_row2_col3, #T_e12c0_row2_col4, #T_e12c0_row3_col0, #T_e12c0_row3_col1, #T_e12c0_row3_col2, #T_e12c0_row3_col3, #T_e12c0_row3_col4, #T_e12c0_row4_col0, #T_e12c0_row4_col1, #T_e12c0_row4_col2, #T_e12c0_row4_col3, #T_e12c0_row4_col4 {\n","  white-space: pre-wrap;\n","}\n","</style>\n","<table id=\"T_e12c0\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_e12c0_level0_col0\" class=\"col_heading level0 col0\" >title</th>\n","      <th id=\"T_e12c0_level0_col1\" class=\"col_heading level0 col1\" >clean_ingredients</th>\n","      <th id=\"T_e12c0_level0_col2\" class=\"col_heading level0 col2\" >cuisine_tag</th>\n","      <th id=\"T_e12c0_level0_col3\" class=\"col_heading level0 col3\" >diet_tag</th>\n","      <th id=\"T_e12c0_level0_col4\" class=\"col_heading level0 col4\" >cooking_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_e12c0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_e12c0_row0_col0\" class=\"data row0 col0\" >Marinated Flank Steak Recipe</td>\n","      <td id=\"T_e12c0_row0_col1\" class=\"data row0 col1\" >pound flank steak c finely minced green onions scallions c dry red wine c soy sauce tbsp salad oil teaspoon sesame seeds teaspoon packed brown sugar teaspoon grnd black pepper teaspoon grnd ginger clove garlic chopped</td>\n","      <td id=\"T_e12c0_row0_col2\" class=\"data row0 col2\" >American</td>\n","      <td id=\"T_e12c0_row0_col3\" class=\"data row0 col3\" >Non-Vegetarian, Gluten-Free</td>\n","      <td id=\"T_e12c0_row0_col4\" class=\"data row0 col4\" >6 hr 0 min</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_e12c0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_e12c0_row1_col0\" class=\"data row1 col0\" >French Chicken Stew</td>\n","      <td id=\"T_e12c0_row1_col1\" class=\"data row1 col1\" >tablespoon rosemary teaspoon thyme bay leaves teaspoon smoked paprika teaspoon pepper cup red wine cups chicken broth cups button mushrooms sliced cups mushroom mix oyster shiitake baby bella sliced medium carrots sliced diagonally onion medium chopped red potato medium cut in inch pieces cup frozen green beans inch pieces can black olives pitted ripe halved handful grape tomatoes halved chicken thighs with bones and skin lbs stalks celery cups water</td>\n","      <td id=\"T_e12c0_row1_col2\" class=\"data row1 col2\" >French</td>\n","      <td id=\"T_e12c0_row1_col3\" class=\"data row1 col3\" >Non-Vegetarian, Gluten-Free</td>\n","      <td id=\"T_e12c0_row1_col4\" class=\"data row1 col4\" >6 hr 0 min</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_e12c0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_e12c0_row2_col0\" class=\"data row2 col0\" >Glazed Carrots</td>\n","      <td id=\"T_e12c0_row2_col1\" class=\"data row2 col1\" >to carrots tbsp butter c brown sugar grated lemon rind and juice</td>\n","      <td id=\"T_e12c0_row2_col2\" class=\"data row2 col2\" >French</td>\n","      <td id=\"T_e12c0_row2_col3\" class=\"data row2 col3\" >Vegetarian, Gluten-Free</td>\n","      <td id=\"T_e12c0_row2_col4\" class=\"data row2 col4\" >15 min</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_e12c0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_e12c0_row3_col0\" class=\"data row3 col0\" >Moms Pie Dough </td>\n","      <td id=\"T_e12c0_row3_col1\" class=\"data row3 col1\" >cups flour tsp salt pinch baking powder tbls sugar cup crisco egg lightly beaten tsp vinegar ice water</td>\n","      <td id=\"T_e12c0_row3_col2\" class=\"data row3 col2\" >American</td>\n","      <td id=\"T_e12c0_row3_col3\" class=\"data row3 col3\" >Vegetarian, Contains Gluten</td>\n","      <td id=\"T_e12c0_row3_col4\" class=\"data row3 col4\" >30 min</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_e12c0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_e12c0_row4_col0\" class=\"data row4 col0\" >Pretzel Salad Or Dessert</td>\n","      <td id=\"T_e12c0_row4_col1\" class=\"data row4 col1\" >c crushed small thin pretzels sticks c margarine</td>\n","      <td id=\"T_e12c0_row4_col2\" class=\"data row4 col2\" >German</td>\n","      <td id=\"T_e12c0_row4_col3\" class=\"data row4 col3\" >Vegan, Gluten-Free</td>\n","      <td id=\"T_e12c0_row4_col4\" class=\"data row4 col4\" >8 min</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["# Define output path\n","output_path = \"/content/drive/MyDrive/NLP - Group Project/tagged_recipes.csv\"\n","\n","# Save the result\n","df_preprocessed.to_csv(output_path, index=False)\n","print(f\"Saved to {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjHEHrepvK_O","executionInfo":{"status":"ok","timestamp":1753822295061,"user_tz":240,"elapsed":127470,"user":{"displayName":"Ayu Putri Vidiantiwi","userId":"02420242710440339287"}},"outputId":"a1d50c15-8412-4543-e525-d439f8a89e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved to /content/drive/MyDrive/NLP - Group Project/tagged_recipes.csv\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}